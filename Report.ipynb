{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff44060-ff2c-45e2-89aa-db40517a41b8",
   "metadata": {},
   "source": [
    "For this week's mini-project, you will participate in this Kaggle competition: \n",
    "Histopathologic Cancer Detection\n",
    "\n",
    "This Kaggle competition is a binary image classification problem where you will identify metastatic cancer in small image patches taken from larger digital pathology scans.\n",
    "\n",
    "You will submit three deliverables: \n",
    "\n",
    "Deliverable 1 — A Jupyter notebook with a description of the problem/data, exploratory data analysis (EDA) procedure, analysis (model building and training), result, and discussion/conclusion. \n",
    "\n",
    "Suppose your work becomes so large that it doesn’t fit into one notebook (or you think it will be less readable by having one large notebook). In that case, you can make several notebooks or scripts in a GitHub repository (as deliverable 3) and submit a report-style notebook or pdf instead. \n",
    "\n",
    "If your project doesn’t fit into Jupyter notebook format (E.g., you built an app that uses ML), write your approach as a report and submit it in a pdf form. \n",
    "\n",
    "Deliverable 2 — A public project GitHub repository with your work (please also include the GitHub repo URL in your notebook/report).\n",
    "\n",
    "Deliverable 3 — A screenshot of your position on the Kaggle competition leaderboard for your top-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d05950-14d5-44e5-b725-0eca6f21d87e",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "We first start by testing a simple CNN using only numpy matrices. Since memory is limited on my system and I was at first having challenges getting CUDA working for linux, I used a small subset of the training data and validated it against some reserved examples. A screenshot is available for the validation accuracy, but this was not used to generate the final submission.\n",
    "\n",
    "The next part of the project I used tensor flow with adam optimizer and binary cross entropy loss function. The model is a simple CNN with 3 convolutional layers and 3 max pooling layers. The model is trained for 10 epochs. I achieved an accuracy ~70% across multiple runs. After saving the model, I used it to generate predictions on the test data and submitted the results to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15e20d-f4fc-41e3-8e9e-d7695ba3f7f0",
   "metadata": {},
   "source": [
    "**This is a report style notebook so no data is provided and this is not intended to run**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83a44d-cd6a-424b-989e-aff681522639",
   "metadata": {},
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c6a47-f14b-44ca-8c39-ac8a196aeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "train_data_path = 'data/train'\n",
    "test_data_path = 'data/test'\n",
    "labels_path = 'data/train_labels.csv'\n",
    "\n",
    "# Load training labels\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "\n",
    "def load_images(data_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.tif'):\n",
    "            img_path = os.path.join(data_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "def process_in_batches(images, batch_size=32):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        yield images[i:i+batch_size]\n",
    "\n",
    "# Load training data\n",
    "train_images = load_images(train_data_path)\n",
    "train_images = train_images[:round(len(train_images) *.20)]\n",
    "train_labels = labels_df['label'].values\n",
    "\n",
    "# Load test data\n",
    "test_images = load_images(test_data_path)\n",
    "\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "print(\"Data Characteristics: \")\n",
    "print(f\"Train images shape: {train_images.shape}  |  Number of Samples: {len(train_images)}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}  |  Number of Samples: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2ee0c-86d0-4650-8ad5-2656e516bc8e",
   "metadata": {},
   "source": [
    "## Simple CNN\n",
    "I wanted to start by handrolling a CNN using only numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39466dbe-1434-4680-8332-3fa3cb288eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \"\"\"\n",
    "        Implements a Convolutional Neural Network (CNN) with basic layers such as 2D convolution, ReLU activation, and max pooling.\n",
    "        \n",
    "        The `CNN` class provides the following methods:\n",
    "        \n",
    "        - `__init__()`: Initializes the layers, weights, and biases of the CNN.\n",
    "        - `conv2d(input, kernel, bias)`: Performs 2D convolution on the input using the provided kernel and bias.\n",
    "        - `relu(input)`: Applies the ReLU activation function to the input.\n",
    "        - `max_pool(input)`: Implements max pooling on the input.\n",
    "        - `forward(input)`: Performs the forward pass of the CNN on the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize layers, weights, and biases\n",
    "        self.conv1_weights = np.random.randn(3, 3, 3, 16).astype(np.float32) * 0.01\n",
    "        self.conv1_bias = np.zeros((16, 1), dtype=np.float32)\n",
    "        self.conv2_weights = np.random.randn(3, 3, 16, 32).astype(np.float32) * 0.01\n",
    "        self.conv2_bias = np.zeros((32, 1), dtype=np.float32)\n",
    "        self.fc_weights = np.random.randn(32 * 22 * 22, 1).astype(np.float32) * 0.01\n",
    "        self.fc_bias = np.zeros((1, 1), dtype=np.float32)\n",
    "\n",
    "    def conv2d(self, input, kernel, bias):\n",
    "        h_out = input.shape[1] - kernel.shape[0] + 1\n",
    "        w_out = input.shape[2] - kernel.shape[1] + 1\n",
    "        output = np.zeros((input.shape[0], h_out, w_out, kernel.shape[3]))\n",
    "        \n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                output[:, i, j, :] = np.sum(input[:, i:i+kernel.shape[0], j:j+kernel.shape[1], :, np.newaxis] * \n",
    "                                            kernel[np.newaxis, :, :, :], axis=(1, 2, 3)) + bias.T\n",
    "        return output\n",
    "\n",
    "    def relu(self, input):\n",
    "        # Implement ReLU activation\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def max_pool(self, input):\n",
    "        h_out, w_out = input.shape[1] // 2, input.shape[2] // 2\n",
    "        output = np.zeros((input.shape[0], h_out, w_out, input.shape[3]))\n",
    "        \n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                output[:, i, j, :] = np.max(input[:, 2*i:2*i+2, 2*j:2*j+2, :], axis=(1, 2))\n",
    "        return output\n",
    "\n",
    "    def forward(self, input):\n",
    "        # First convolutional layer\n",
    "        conv1 = self.conv2d(input, self.conv1_weights, self.conv1_bias)\n",
    "        relu1 = self.relu(conv1)\n",
    "        pool1 = self.max_pool(relu1)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        conv2 = self.conv2d(pool1, self.conv2_weights, self.conv2_bias)\n",
    "        relu2 = self.relu(conv2)\n",
    "        pool2 = self.max_pool(relu2)\n",
    "        \n",
    "        # Flatten and fully connected layer\n",
    "        flattened = pool2.reshape(pool2.shape[0], -1)\n",
    "        output = np.dot(flattened, self.fc_weights) + self.fc_bias.T\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f412fa-0b52-419f-aab9-484af61f047f",
   "metadata": {},
   "source": [
    "Since system memory was an issue on my workstation, I had to use batch processing on a subset of the image data, and then validated on a reserved subset of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17673a95-ba16-4890-897d-9deea79835d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(images, batch_size=32):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        yield images[i:i+batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bec714-2709-4465-860a-66107dcd4f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Shuffle and split the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(train_images))\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n\u001b[1;32m      4\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.75\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_images))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Shuffle and split the data\n",
    "indices = np.arange(len(train_images))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.75 * len(train_images))\n",
    "\n",
    "train_indices = indices[:split]\n",
    "val_indices = indices[split:]\n",
    "\n",
    "X_train = train_images[train_indices]\n",
    "y_train = train_labels[train_indices]\n",
    "X_val = train_images[val_indices]\n",
    "y_val = train_labels[val_indices]\n",
    "\n",
    "# Create CNN instance\n",
    "import simple_cnn\n",
    "cnn = simple_cnn.CNN()\n",
    "\n",
    "batch_predictions = []\n",
    "i = 0\n",
    "for batch in process_in_batches(train_images):\n",
    "    print(f\"Batch {i} of {len(train_images) // 32}\")\n",
    "    batch_pred = cnn.forward(batch)\n",
    "    batch_predictions.append(batch_pred)\n",
    "    i += 1\n",
    "\n",
    "predictions = np.concatenate(batch_predictions, axis=0)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions = cnn.forward(X_val)\n",
    "val_predictions = (val_predictions > 0.5).astype(int)  # Assuming binary classification\n",
    "\n",
    "accuracy = np.mean(val_predictions == y_val)\n",
    "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561b79d-f899-474f-8c22-f9913594d2c9",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695435f9-ec1d-46a4-a06e-b9b424a0928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 14:58:23.247292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-28 14:58:23.265238: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-28 14:58:23.270384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-28 14:58:23.283950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 14:58:24.006952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, 3, activation='relu')\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7183e6-07d7-4329-8318-bd5989fddbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_images = load_images(train_data_path)\n",
    "train_images = train_images[:round(len(train_images) *.20)]\n",
    "train_labels = labels_df['label'].values\n",
    "\n",
    "# Load test data\n",
    "test_images = load_images(test_data_path)\n",
    "\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "print(\"Data Characteristics: \")\n",
    "print(f\"Train images shape: {train_images.shape}  |  Number of Samples: {len(train_images)}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}  |  Number of Samples: {len(test_images)}\")\n",
    "\n",
    "# Create and compile the model\n",
    "model = tf_cnn.CNN()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_split=0.25, batch_size=32)\n",
    "\n",
    "# After training the model\n",
    "model.save('cancer_detection_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68820ba-828b-49ca-923b-ff1aef5f508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model and generate predictions on test data\n",
    "\n",
    "# Generate predictions\n",
    "predictions = loaded_model.predict(test_images)\n",
    "predictions = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "test_filenames = [f.split('.')[0] for f in os.listdir(test_data_path) if f.endswith('.tif')]\n",
    "results_df = pd.DataFrame({\n",
    "    'id': test_filenames,\n",
    "    'label': predictions\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c0a53-1d95-4594-baf7-0805f1e9583c",
   "metadata": {},
   "source": [
    "## Results and Refinements\n",
    "Since I had a memory issue on my machine and likely need to straighten out GPU programming, I could only train on a subset of data, yet the model still achieved  ~70% accuracy on the test data. The simple CNN not making use of hardware acceleration highlights the importance of parallelization when training models. Each batch took about ~5 seconds to run, and there were hundreds of batches even only using a subset of the data. When running with TF, I could train on a much larger set of the data and achieve better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f95946-20ff-4187-8bb7-1aba69c014b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
